{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_trainsition_prob = [\n",
    "    [0.721, 0.072, 0.207],\n",
    "    [0.296, 0.142, 0.563],\n",
    "    [0.114, 0.058, 0.828]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1_trainsition_prob = [\n",
    "    [0.2143, 0.144 , 0.6417],\n",
    "    [0,0,1],\n",
    "    [0,0,1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0021399999999998087"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (0.563 * 1.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 2.0 # increase rate of (0,1)\n",
    "beta = 3.1 # increase rate of (0,2) -> Intervention A\n",
    "gamma = 1.78 # increase rate of (1,2) \n",
    "theta = 1.21 # increase rate of (2,2)\n",
    "\n",
    "# after increase each different prob decrease by \"minus\" (not proportion)\n",
    "# [0.721-P, 0.072 * alpha, 0.207 * beta]\n",
    "# [0.296-P/2, 0.142-P/2, 0.563 * gamma]\n",
    "# [0.114-P/2, 0.058-P/2, 0.828 * theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    's1' : {\n",
    "        'a1' : {\"s1\": init_trainsition_prob[0][0],\n",
    "                 \"s2\" : init_trainsition_prob[0][1],\n",
    "                 \"s3\" : init_trainsition_prob[0][2]},\n",
    "        'a2' : {\"s1\": a1_trainsition_prob[0][0],\n",
    "                 \"s2\" : a1_trainsition_prob[0][1],\n",
    "                 \"s3\" : a1_trainsition_prob[0][2]}\n",
    "    },\n",
    "    's2' : {\n",
    "        'a1' : {\"s1\": init_trainsition_prob[1][0],\n",
    "                 \"s2\" : init_trainsition_prob[1][1],\n",
    "                 \"s3\" : init_trainsition_prob[1][2]},\n",
    "        'a2' : {\n",
    "                 \"s3\" : a1_trainsition_prob[1][2]}\n",
    "    },\n",
    "    's3' : {\n",
    "        'a1' : {\"s1\": init_trainsition_prob[2][0],\n",
    "                 \"s2\" : init_trainsition_prob[2][1],\n",
    "                 \"s3\" : init_trainsition_prob[2][2]},\n",
    "        'a2' : {\n",
    "                 \"s3\" : a1_trainsition_prob[1][2]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Environment.Model import MarkovDecisionProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Benetfit(state dependent) - cost(action dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MarkovDecisionProcess(env=env, rewards={'s1': 0, 's2' :0, 's3':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Agent.DecisionMaker import DecisionMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm = DecisionMaker(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MarkovDecisionProcess.get_reward of <Environment.Model.MarkovDecisionProcess object at 0x0000028F3ADD3198>>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'s1': 85.91878237318117, 's2': 89.9140495544283, 's3': 99.9140495544283},\n",
       " {'s1': 'a2', 's2': 'a2', 's3': 'a2'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.ValueIteration(discount=0.9, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_py35",
   "language": "python",
   "name": "ml_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
